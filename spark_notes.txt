https://spark.apache.org/
https://spark.apache.org/docs/latest/sql-programming-guide.html


## What is Big Data?
- Data 0-32 GB or 64 GB RAM is not big data
- for larger set of data:
  sql database to move to hard drive instead of RAM
  or, use a distributed systems - distribute data to multiple machines/computer connected through a n/w

# Distrubuted systems
- scaling is easy - just add more machines to distributed systems
- fault tolerance

# Hadoop
- distribute very large files across multiple machines
- uses Hadoop Distributed File System (HDFS)
- work with large data sets
- duplicates / replicates data
- uses MapReduce - allows computations on that data

                     ---- Data Node <CPU|RAM>
Name Node <CPU|RAM>  ---- Data Node <CPU|RAM>
                     ---- Data Node <CPU|RAM>

# HDFS
- use blocks of data - 128 MB default
- replicated 3 times to support fault tolerance
- smaller blocks provide more parallelization during processing

# MapReduce
- splitting a computational task to a distributed set of files (like HDFS)
- consists of a Job Tracker and multiple Task Trackers
- Job Tracker sends code to run on the Task Trackers
- Task Trackers allocate CPU and memory for the tasks and monitor the tasks on the worker nodes

------ Using HDFS to distribute large data sets
------ Using MapReduce to distribute a computational task to a distributed data set


## Spark
- flexible alternative to MapReduce
- can use data stored in different formats like:
  Cassandra
  AWS S3
  HDFS
  and more...

# Spark vs MapReduce
- MapReduce requires files to be stored in HDFS
- Spark is 100x faster
- MapReduce writes most data to disk after each map and reduce operation
- Spark keeps most of the data in memory after each transformation
- Spark can spill over to disk if the memory is filled

# Spark RDDs
- idea of a Resilient Distributed Dataset (RDD)
- RDD has 4 main features:
  Distributed collection of data
  Fault-tolerant
  Parallel operation - partitioned
  Ability to use many data sources
- RDDs are immutable, lazily evaluated, cacheable
- 2 types of Spark operations:
  Transformations: basically a recipe to follow
  Actions: perform what the recipe says to do and returns something back
- latest version is moving towards a DataFrame based syntax, but the idea is based on RDD only


##################################
# Installation - Python and Spark
##################################
- only works on Linux
- 4 methods to setup:
  Ubuntu + Spark + Python on VirtualBox
  Amazon EC2 with Python and Spark
  Databricks Notebook System
  AWS EMR Notebook (not free!)

- best method to setup is as a docker image

## Databricks Notebook System
- free community version that supports a 15GB cluster
- own storage format known as DBFS
- this table format needs to be accessed in a particular way
- link - Cy32th@abc
https://databricks.com/try-databricks



######################
# Python crash course
######################

## setup Visual code for python
- install extensions:
Python Intellisense(Pylance), Lint.....
Python Docstring Generator

- Command Palette -> Linter -> Select Python Linter -> pylint


## Jupyter notebook
- when type object variable - use <dot(.) and Tab> to look for all methods
- for any function - use <Shift + Tab> to look for Docstring
- in markdown - use multiple '#' to increase or decrease the heading


## Crash Course 1
- type(1.0)
float

- string can be single quote or double quote - however, python displays it as single quote for both

# append vs extend - list
- mylist.append('something') - to add an item to a list
>>> my_list
['foo', 'bar', 'baz']
>>> another_list = [1, 2, 3]
>>> my_list.append(another_list)
>>> my_list
['foo', 'bar', 'baz', [1, 2, 3]]
                     #^^^^^^^^^--- single item at the end of the list.

- mylist.extend([1,2,3]) will add each item individually
>>> my_list
['foo', 'bar']
>>> another_list = [1, 2, 3]
>>> my_list.extend(another_list)
>>> my_list
['foo', 'bar', 1, 2, 3]


- a dictionary: d = {'k1':'v1', 'k2':'v2'}
- no order maintained - same as Java HashMap
- 0 is False and non-zero is True
- a tuple: tup = (1,2,3)
- a set: myset = {1,2,3}


## Crash Course 2
- use and, or, not to combine boolean expressions
(1 == 1) and ('a' == "a")

- if-else example
if 1==2:
  print("yeah!")
elif 3==3:
  print("nice")
else:
  print("nope")

- for loop example
for jelly in seq:
  print(jelly)

- while loop example
i = 1
while i < 5:
  print(f'i is {i}')
  i = i+1

- can create a list from range
list(range(5))

output:
[0, 1, 2, 3, 4]

for x in range(5):
  print(x)

- other examples in notebook

- list comprehension
[ x**2 for x in range(5) ]


## Crash course 3

- function
def my_func(name='no name'):
  print('hello ' + name)

def square(x):
  return x**2

- lambda
lambda var: var*2

- split
tweet = 'Hello #sports'
tweet.split('#')

- in
'x' in [1,2,3]
False

## Assignment
- Python has str.__contains__() method similar to Java string contains()
- could use 'in' instead of contains
return 'dog' in st.lower()
- can use shortcut
count = count + 1
count += 1



